# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import base64
import boto3
import json
import time
from pathlib import Path
from datetime import datetime
from statistics import mean, median
from collections import defaultdict


userLoc = 'Chinese'
userLabels = '{"1":"person wear glasses", "2":"cat is sleeping", "3":"child"}'

# Create a Bedrock Runtime client
client = boto3.client(
    "bedrock-runtime",
    region_name="us-west-2"
)

# å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹ IDï¼ˆä¸åŒåœ°ç†åŒºåŸŸçš„ inference profileï¼‰
MODEL_IDS = {
    "global": "global.amazon.nova-2-lite-v1:0",
    "us": "us.amazon.nova-2-lite-v1:0",
    "eu": "eu.amazon.nova-2-lite-v1:0",
    "jp": "jp.amazon.nova-2-lite-v1:0"
}

# å®šä¹‰è¦æµ‹è¯•çš„ service tiers
SERVICE_TIERS = ["flex", "default", "priority"]

# å®šä¹‰å›¾ç‰‡ç›®å½•
image_dir = "./images"

# æ¯ä¸ªæ¨¡å‹å’Œ tier ç»„åˆçš„æ€§èƒ½ç»Ÿè®¡
tier_stats = {f"{model_type}_{tier}": {
    "model_type": model_type,
    "tier": tier,
    "total_images": 0,
    "successful": 0,
    "failed": 0,
    "latencies_ms": [],
    "input_tokens": [],
    "output_tokens": [],
    "errors": []
} for model_type in MODEL_IDS.keys() for tier in SERVICE_TIERS}

# Define your system prompt(s)
extra_instruction = f"Translation in locale {userLoc} language"

system_list = [{
    "text": "You are a surveillance image analyst. Analyze images and output ONLY valid JSON."
}]

# è·å–ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}
image_files = [f for f in Path(image_dir).iterdir() if f.suffix.lower() in image_extensions]

print(f"{'='*80}")
print(f"Service Tier æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
print(f"{'='*80}")
print(f"æµ‹è¯•æ¨¡å‹: Global (å…¨çƒ) / US (ç¾å›½) / EU (æ¬§æ´²) / JP (æ—¥æœ¬)")
print(f"æµ‹è¯•å›¾ç‰‡æ•°: {len(image_files)}")
print(f"æµ‹è¯• Tiers: {', '.join(SERVICE_TIERS)}")
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"{'='*80}\n")

# å¯¹æ¯ä¸ªæ¨¡å‹ç±»å‹å’Œ tier è¿›è¡Œæµ‹è¯•
for model_type, model_id in MODEL_IDS.items():
    print(f"\n{'='*80}")
    print(f"æµ‹è¯•æ¨¡å‹ç±»å‹: {model_type.upper()}")
    print(f"æ¨¡å‹ ID: {model_id}")
    print(f"{'='*80}\n")
    
    for tier in SERVICE_TIERS:
        stats_key = f"{model_type}_{tier}"
        print(f"\n{'â”€'*80}")
        print(f"  Service Tier: {tier.upper()}")
        print(f"  å®šä»·: {'0.5x (50% æŠ˜æ‰£)' if tier == 'flex' else '1.0x (åŸºå‡†)' if tier == 'default' else '1.75x (75% æº¢ä»·)'}")
        print(f"{'â”€'*80}\n")
        
        tier_stats[stats_key]["total_images"] = len(image_files)
    
        # éå†å¤„ç†æ¯å¼ å›¾ç‰‡
        for idx, image_path in enumerate(image_files, 1):
            print(f"  [{idx}/{len(image_files)}] {image_path.name}", end=" ... ")
            
            try:
                # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
                with open(image_path, "rb") as image_file:
                    binary_data = image_file.read()
                    base_64_encoded_data = base64.b64encode(binary_data)
                    base64_string = base_64_encoded_data.decode("utf-8")
                
                # è·å–å›¾ç‰‡æ ¼å¼
                image_format = image_path.suffix.lower().replace('.', '')
                if image_format == 'jpg':
                    image_format = 'jpeg'
            
                # Define a "user" message
                message_list = [
                    {
                        "role": "user",
                        "content": [
                            {
                                "image": {
                                    "format": image_format,
                                    "source": {"bytes": base64_string},
                                }
                            },
                            {
                                "text": f"""Analyze this image and output ONLY valid JSON.

## OUTPUT FORMAT (Required)
{{
  "description": "[Concise English description â‰¤100 chars: natural scene description, avoid unnecessary articles like 'a/an' before person/people/objects]",
  "descriptionExtra": "[{extra_instruction}]",
  "keys": ["matched scene labels from SCENES INPUT only"],
  "risk": "[Safety risk description or empty string]",
  "noDetection": "[Set 'false' if ANY person/animal/vehicle detected, otherwise set 'true']",
  "summary": "[Natural English summary â‰¤30 chars, conversational tone, capitalize first letter, no punctuation]",
  "summaryExtra": "[{extra_instruction}]"
}}

## CRITICAL RULES
1. Detection: Only count real physical presence, NOT reflections/shadows
   - Reflections on windows â†’ DO NOT count as detection
   - Set noDetection="false" only when real person/animal/vehicle detected
2. keys Matching: Match image against SCENES INPUT descriptions, return corresponding key IDs only
   - If SCENES INPUT is empty or uncertain â†’ return `"keys": []`
   - NEVER create new keys not in SCENES INPUT
3. Language: Keep description/summary in English
4. Style: Use "person" not "a person", be concise and direct

Locale: {userLoc}
SCENES INPUT: {userLabels}

Do NOT ever put escaped Unicode in the output - just use the unescaped native character, for example, do not include sequences such as \u3492.

Examples:
- Image: Man with glasses + INPUT: {{"1":"person wear glasses"}} â†’ keys: ["1"]
- Image: Sleeping cat + INPUT: {{"2":"cat is sleeping", "3":"dog"}} â†’ keys: ["2"]
- Image: Dog playing + INPUT: {{"5":"child"}} â†’ keys: []
"""
                            }
                        ],
                    }
                ]
                
                # Configure the inference parameters
                inf_params = {"maxTokens": 3000, "topP": 0.8, "temperature": 0.1, "topK": 15}

                native_request = {
                    "schemaVersion": "messages-v1",
                    "messages": message_list,
                    "system": system_list,
                    "inferenceConfig": inf_params,
                }
                
                # æ„å»ºè°ƒç”¨å‚æ•°
                invoke_params = {
                    "modelId": model_id,
                    "body": json.dumps(native_request),
                    "accept": "application/json",
                    "contentType": "application/json"
                }
                
                # åªæœ‰é default æ—¶æ‰æ·»åŠ  serviceTier å‚æ•°
                if tier != "default":
                    invoke_params["serviceTier"] = tier
                
                response = client.invoke_model(**invoke_params)
                
                model_response = json.loads(response["body"].read())
                
                # æå–æ€§èƒ½æŒ‡æ ‡
                content_text = model_response["output"]["message"]["content"][0]["text"]
                usage = model_response.get("usage", {})
                input_tokens = usage.get("inputTokens", 0)
                output_tokens = usage.get("outputTokens", 0)
                
                # ä» HTTP å“åº”å¤´è·å–çœŸå®çš„å»¶è¿ŸæŒ‡æ ‡
                http_headers = response.get("ResponseMetadata", {}).get("HTTPHeaders", {})
                latency_ms = int(http_headers.get("x-amzn-bedrock-invocation-latency", 0))
                actual_tier = http_headers.get("x-amzn-bedrock-service-tier", tier)
                
                # è®°å½•ç»Ÿè®¡
                tier_stats[stats_key]["successful"] += 1
                tier_stats[stats_key]["latencies_ms"].append(latency_ms)
                tier_stats[stats_key]["input_tokens"].append(input_tokens)
                tier_stats[stats_key]["output_tokens"].append(output_tokens)
                
                print(f"âœ“ {latency_ms}ms | tokens(in:{input_tokens} out:{output_tokens}) | tier:{actual_tier}")
                
            except Exception as e:
                tier_stats[stats_key]["failed"] += 1
                tier_stats[stats_key]["errors"].append(str(e))
                print(f"âœ— é”™è¯¯: {str(e)}")
        
        # æ‰“å°è¯¥ tier çš„ç®€è¦ç»Ÿè®¡
        if tier_stats[stats_key]["latencies_ms"]:
            print(f"\n  {tier.upper()} Tier ç»Ÿè®¡:")
            print(f"    å¹³å‡å»¶è¿Ÿ: {mean(tier_stats[stats_key]['latencies_ms']):.0f}ms")
            print(f"    ä¸­ä½æ•°å»¶è¿Ÿ: {median(tier_stats[stats_key]['latencies_ms']):.0f}ms")

# æ‰“å°å¯¹æ¯”æŠ¥å‘Š
print(f"\n\n{'='*80}")
print(f"Service Tier æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
print(f"{'='*80}")
print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
print(f"{'æ¨¡å‹':<10} {'Tier':<12} {'å®šä»·':<15} {'æˆåŠŸ':<8} {'å¹³å‡å»¶è¿Ÿ':<12} {'ä¸­ä½æ•°å»¶è¿Ÿ':<12} {'å¹³å‡Token':<12}")
print(f"{'-'*90}")

for model_type in MODEL_IDS.keys():
    for tier in SERVICE_TIERS:
        stats_key = f"{model_type}_{tier}"
        stats = tier_stats[stats_key]
        pricing = "0.5x (50%â†“)" if tier == "flex" else "1.0x (åŸºå‡†)" if tier == "default" else "1.75x (75%â†‘)"
        
        if stats["latencies_ms"]:
            avg_latency = mean(stats["latencies_ms"])
            median_latency = median(stats["latencies_ms"])
            avg_tokens = mean(stats["input_tokens"]) + mean(stats["output_tokens"])
            
            print(f"{model_type:<10} {tier:<12} {pricing:<15} {stats['successful']:<8} {avg_latency:<12.0f} {median_latency:<12.0f} {avg_tokens:<12.0f}")

# è¯¦ç»†ç»Ÿè®¡
print(f"\n{'='*80}")
print(f"è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
print(f"{'='*80}\n")

for model_type in MODEL_IDS.keys():
    for tier in SERVICE_TIERS:
        stats_key = f"{model_type}_{tier}"
        stats = tier_stats[stats_key]
        pricing = "0.5x (50% æŠ˜æ‰£)" if tier == "flex" else "1.0x (åŸºå‡†)" if tier == "default" else "1.75x (75% æº¢ä»·)"
        
        print(f"## {model_type.upper()} - {tier.upper()} Tier - {pricing}")
        print(f"{'â”€'*80}")
    
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
    print(f"  æˆåŠŸ: {stats['successful']}")
    print(f"  å¤±è´¥: {stats['failed']}")
    print(f"  æˆåŠŸç‡: {stats['successful']/stats['total_images']*100:.1f}%")
    
    if stats["latencies_ms"]:
        print(f"\nâš¡ Bedrock å»¶è¿Ÿç»Ÿè®¡ (æœåŠ¡ç«¯):")
        print(f"  å¹³å‡å»¶è¿Ÿ: {mean(stats['latencies_ms']):.0f}ms")
        print(f"  ä¸­ä½æ•°: {median(stats['latencies_ms']):.0f}ms")
        print(f"  æœ€å¿«: {min(stats['latencies_ms'])}ms")
        print(f"  æœ€æ…¢: {max(stats['latencies_ms'])}ms")
    
    if stats["input_tokens"]:
        print(f"\nğŸ”¤ Token ç»Ÿè®¡:")
        print(f"  è¾“å…¥ Token - å¹³å‡: {mean(stats['input_tokens']):.0f}")
        print(f"  è¾“å…¥ Token - æ€»è®¡: {sum(stats['input_tokens'])}")
        print(f"  è¾“å‡º Token - å¹³å‡: {mean(stats['output_tokens']):.0f}")
        print(f"  è¾“å‡º Token - æ€»è®¡: {sum(stats['output_tokens'])}")
        print(f"  æ€» Token: {sum(stats['input_tokens']) + sum(stats['output_tokens'])}")
    
    if stats["errors"]:
        print(f"\nâŒ é”™è¯¯åˆ—è¡¨:")
        for i, error in enumerate(stats["errors"], 1):
            print(f"  {i}. {error}")
        
        print()

# æ€§èƒ½å¯¹æ¯”åˆ†æ
print(f"{'='*80}")
print(f"æ€§èƒ½å¯¹æ¯”åˆ†æ")
print(f"{'='*80}\n")

for model_type in MODEL_IDS.keys():
    print(f"\n## {model_type.upper()} æ¨¡å‹å¯¹æ¯”")
    print(f"{'â”€'*80}")
    
    stats_keys = [f"{model_type}_{tier}" for tier in SERVICE_TIERS]
    if all(tier_stats[key]["latencies_ms"] for key in stats_keys):
        default_avg = mean(tier_stats[f"{model_type}_default"]["latencies_ms"])
        flex_avg = mean(tier_stats[f"{model_type}_flex"]["latencies_ms"])
        priority_avg = mean(tier_stats[f"{model_type}_priority"]["latencies_ms"])
        
        print(f"âš¡ Bedrock å»¶è¿Ÿå¯¹æ¯” (ç›¸å¯¹äº Default):")
        print(f"  Flex:     {flex_avg:.0f}ms ({(flex_avg/default_avg-1)*100:+.1f}%)")
        print(f"  Default:  {default_avg:.0f}ms (åŸºå‡†)")
        print(f"  Priority: {priority_avg:.0f}ms ({(priority_avg/default_avg-1)*100:+.1f}%)")
        
        print(f"\nğŸ’° æ€§ä»·æ¯”åˆ†æ (é€Ÿåº¦/ä»·æ ¼):")
        # å»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨ 1/latency
        flex_value = (1000/flex_avg) / 0.5
        default_value = (1000/default_avg) / 1.0
        priority_value = (1000/priority_avg) / 1.75
        
        print(f"  Flex:     {flex_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        print(f"  Default:  {default_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        print(f"  Priority: {priority_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        
        best_value = max(flex_value, default_value, priority_value)
        if best_value == flex_value:
            print(f"\nğŸ† {model_type.upper()} æœ€ä½³æ€§ä»·æ¯”: Flex Tier")
        elif best_value == default_value:
            print(f"\nğŸ† {model_type.upper()} æœ€ä½³æ€§ä»·æ¯”: Default Tier")
        else:
            print(f"\nğŸ† {model_type.upper()} æœ€ä½³æ€§ä»·æ¯”: Priority Tier")

print(f"\n{'='*80}")
