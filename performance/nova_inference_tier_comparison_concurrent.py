# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import base64
import boto3
import json
import time
from pathlib import Path
from datetime import datetime
from statistics import mean, median
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# å¹¶å‘é…ç½® - 50 å¹¶å‘è¯·æ±‚
CONCURRENCY = 50

userLoc = 'Chinese'
userLabels = '{"1":"person wear glasses", "2":"cat is sleeping", "3":"child"}'

# Create a Bedrock Runtime client
client = boto3.client(
    "bedrock-runtime",
    region_name="us-west-2"
)

# åªæµ‹è¯•ç¾å›½çš„ inference profile
MODEL_ID = "us.amazon.nova-2-lite-v1:0"

# å®šä¹‰è¦æµ‹è¯•çš„ service tiers
SERVICE_TIERS = ["flex", "default", "priority"]

# å®šä¹‰å›¾ç‰‡ç›®å½•
image_dir = "./images"

# è·å–ç›®å½•ä¸‹æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}
test_images = [f for f in Path(image_dir).iterdir() if f.suffix.lower() in image_extensions]

if not test_images:
    raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ ({image_dir})")

print(f"æ‰¾åˆ° {len(test_images)} å¼ æµ‹è¯•å›¾ç‰‡: {[img.name for img in test_images]}")

# çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡æ•°æ® - æŒ‰ tier ç»„ç»‡
stats_lock = threading.Lock()
tier_stats = {tier: {
    "tier": tier,
    "total_requests": 0,
    "successful": 0,
    "failed": 0,
    "latencies_ms": [],
    "input_tokens": [],
    "output_tokens": [],
    "errors": defaultdict(int),
    "total_time": 0
} for tier in SERVICE_TIERS}

# è¿›åº¦è®¡æ•°å™¨
progress_counter = {"count": 0}
progress_lock = threading.Lock()

# Define your system prompt(s)
extra_instruction = f"Translation in locale {userLoc} language"

system_list = [{
    "text": "You are a surveillance image analyst. Analyze images and output ONLY valid JSON."
}]

def process_single_request(tier, image_path, request_id):
    """å¤„ç†å•ä¸ªè¯·æ±‚"""
    try:
        # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
        with open(image_path, "rb") as image_file:
            binary_data = image_file.read()
            base_64_encoded_data = base64.b64encode(binary_data)
            base64_string = base_64_encoded_data.decode("utf-8")
        
        # è·å–å›¾ç‰‡æ ¼å¼
        image_format = image_path.suffix.lower().replace('.', '')
        if image_format == 'jpg':
            image_format = 'jpeg'
    
        # Define a "user" message
        message_list = [
            {
                "role": "user",
                "content": [
                    {
                        "image": {
                            "format": image_format,
                            "source": {"bytes": base64_string},
                        }
                    },
                    {
                        "text": f"""Analyze this image and output ONLY valid JSON.

## OUTPUT FORMAT (Required)
{{
  "description": "[Concise English description â‰¤100 chars: natural scene description, avoid unnecessary articles like 'a/an' before person/people/objects]",
  "descriptionExtra": "[{extra_instruction}]",
  "keys": ["matched scene labels from SCENES INPUT only"],
  "risk": "[Safety risk description or empty string]",
  "noDetection": "[Set 'false' if ANY person/animal/vehicle detected, otherwise set 'true']",
  "summary": "[Natural English summary â‰¤30 chars, conversational tone, capitalize first letter, no punctuation]",
  "summaryExtra": "[{extra_instruction}]"
}}

## CRITICAL RULES
1. Detection: Only count real physical presence, NOT reflections/shadows
   - Reflections on windows â†’ DO NOT count as detection
   - Set noDetection="false" only when real person/animal/vehicle detected
2. keys Matching: Match image against SCENES INPUT descriptions, return corresponding key IDs only
   - If SCENES INPUT is empty or uncertain â†’ return `"keys": []`
   - NEVER create new keys not in SCENES INPUT
3. Language: Keep description/summary in English
4. Style: Use "person" not "a person", be concise and direct

Locale: {userLoc}
SCENES INPUT: {userLabels}

Do NOT ever put escaped Unicode in the output - just use the unescaped native character, for example, do not include sequences such as \u3492.

Examples:
- Image: Man with glasses + INPUT: {{"1":"person wear glasses"}} â†’ keys: ["1"]
- Image: Sleeping cat + INPUT: {{"2":"cat is sleeping", "3":"dog"}} â†’ keys: ["2"]
- Image: Dog playing + INPUT: {{"5":"child"}} â†’ keys: []
"""
                    }
                ],
            }
        ]
        
        # Configure the inference parameters
        inf_params = {"maxTokens": 3000, "topP": 0.8, "temperature": 0.1, "topK": 15}

        native_request = {
            "schemaVersion": "messages-v1",
            "messages": message_list,
            "system": system_list,
            "inferenceConfig": inf_params,
        }
        
        # æ„å»ºè°ƒç”¨å‚æ•°
        invoke_params = {
            "modelId": MODEL_ID,
            "body": json.dumps(native_request),
            "accept": "application/json",
            "contentType": "application/json"
        }
        
        # åªæœ‰é default æ—¶æ‰æ·»åŠ  serviceTier å‚æ•°
        if tier != "default":
            invoke_params["serviceTier"] = tier
        
        response = client.invoke_model(**invoke_params)
        
        model_response = json.loads(response["body"].read())
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        usage = model_response.get("usage", {})
        input_tokens = usage.get("inputTokens", 0)
        output_tokens = usage.get("outputTokens", 0)
        
        # ä» HTTP å“åº”å¤´è·å–çœŸå®çš„å»¶è¿ŸæŒ‡æ ‡
        http_headers = response.get("ResponseMetadata", {}).get("HTTPHeaders", {})
        latency_ms = int(http_headers.get("x-amzn-bedrock-invocation-latency", 0))
        actual_tier = http_headers.get("x-amzn-bedrock-service-tier", tier)
        
        # æ›´æ–°è¿›åº¦
        with progress_lock:
            progress_counter["count"] += 1
            if progress_counter["count"] % 50 == 0:
                print(f"  è¿›åº¦: {progress_counter['count']} è¯·æ±‚å®Œæˆ...")
        
        return {
            "success": True,
            "tier": tier,
            "latency_ms": latency_ms,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "actual_tier": actual_tier
        }
        
    except Exception as e:
        # æ›´æ–°è¿›åº¦
        with progress_lock:
            progress_counter["count"] += 1
            if progress_counter["count"] % 50 == 0:
                print(f"  è¿›åº¦: {progress_counter['count']} è¯·æ±‚å®Œæˆ...")
        
        return {
            "success": False,
            "tier": tier,
            "error": str(e)
        }

print(f"\n{'='*80}")
print(f"Service Tier å¹¶å‘æ€§èƒ½æµ‹è¯•")
print(f"{'='*80}")
print(f"æµ‹è¯•æ¨¡å‹: US (ç¾å›½) - {MODEL_ID}")
print(f"æµ‹è¯•å›¾ç‰‡: {len(test_images)} å¼  ({', '.join([img.name for img in test_images])})")
print(f"å¹¶å‘çº§åˆ«: {CONCURRENCY}")
print(f"æµ‹è¯• Tiers: {', '.join(SERVICE_TIERS)}")
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"{'='*80}\n")

# å¯¹æ¯ä¸ª tier è¿›è¡Œæµ‹è¯•
for tier in SERVICE_TIERS:
        print(f"\n{'â”€'*80}")
        print(f"  Service Tier: {tier.upper()}")
        print(f"  å®šä»·: {'0.5x (50% æŠ˜æ‰£)' if tier == 'flex' else '1.0x (åŸºå‡†)' if tier == 'default' else '1.75x (75% æº¢ä»·)'}")
        print(f"{'â”€'*80}\n")
        
        stats_key = f"{concurrency}_{tier}"
        
        # é‡ç½®è¿›åº¦è®¡æ•°å™¨
        with progress_lock:
            progress_counter["count"] = 0
        
        # åˆ›å»ºè¯·æ±‚åˆ—è¡¨ (å¾ªç¯ä½¿ç”¨5å¼ ä¸åŒçš„å›¾ç‰‡)
        requests = []
        for i in range(concurrency):
            # ä½¿ç”¨æ¨¡è¿ç®—å¾ªç¯é€‰æ‹©å›¾ç‰‡
            image_to_use = test_images[i % len(test_images)]
            requests.append((tier, image_to_use, f"{image_to_use.name}_{i}"))
        
        total_requests = len(requests)
        tier_stats[stats_key]["total_requests"] = total_requests
    
        print(f"  æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"  å¼€å§‹å¹¶å‘æµ‹è¯•...\n")
        
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [executor.submit(process_single_request, *req) for req in requests]
            
            for future in as_completed(futures):
                result = future.result()
                
                with stats_lock:
                    if result["success"]:
                        tier_stats[stats_key]["successful"] += 1
                        tier_stats[stats_key]["latencies_ms"].append(result["latency_ms"])
                        tier_stats[stats_key]["input_tokens"].append(result["input_tokens"])
                        tier_stats[stats_key]["output_tokens"].append(result["output_tokens"])
                    else:
                        tier_stats[stats_key]["failed"] += 1
                        tier_stats[stats_key]["errors"][result["error"]] += 1
        
        end_time = time.time()
        total_time = end_time - start_time
        tier_stats[stats_key]["total_time"] = total_time
        
        # æ‰“å°è¯¥ tier çš„ç»Ÿè®¡
        print(f"\n  {tier.upper()} Tier å®Œæˆ:")
        print(f"    æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"    æˆåŠŸ: {tier_stats[stats_key]['successful']}")
        print(f"    å¤±è´¥: {tier_stats[stats_key]['failed']}")
        if tier_stats[stats_key]["latencies_ms"]:
            latencies = tier_stats[stats_key]['latencies_ms']
            latencies_sorted = sorted(latencies)
            p95_idx = int(len(latencies_sorted) * 0.95)
            p99_idx = int(len(latencies_sorted) * 0.99)
            
            print(f"    å¹³å‡å»¶è¿Ÿ: {mean(latencies):.0f}ms")
            print(f"    ä¸­ä½æ•°å»¶è¿Ÿ: {median(latencies):.0f}ms")
            print(f"    P95 å»¶è¿Ÿ: {latencies_sorted[p95_idx]}ms")
            print(f"    P99 å»¶è¿Ÿ: {latencies_sorted[p99_idx]}ms")
            print(f"    æœ€å¿«: {min(latencies)}ms")
            print(f"    æœ€æ…¢: {max(latencies)}ms")
        
        # æ¯è½®æµ‹è¯•åç­‰å¾…80ç§’
        if tier != SERVICE_TIERS[-1] or concurrency != CONCURRENCY_LEVELS[-1]:
            print(f"\n  â³ ç­‰å¾… 80 ç§’åç»§ç»­ä¸‹ä¸€è½®æµ‹è¯•...")
            time.sleep(80)

# æ‰“å°å¯¹æ¯”æŠ¥å‘Š
print(f"\n\n{'='*80}")
print(f"Service Tier å¹¶å‘æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
print(f"{'='*80}")
print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# æŒ‰å¹¶å‘çº§åˆ«åˆ†ç»„æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
for concurrency in CONCURRENCY_LEVELS:
    print(f"\n## å¹¶å‘çº§åˆ«: {concurrency}")
    print(f"{'â”€'*80}")
    print(f"{'Tier':<12} {'å®šä»·':<15} {'æˆåŠŸç‡':<10} {'å¹³å‡å»¶è¿Ÿ':<12} {'P95å»¶è¿Ÿ':<12} {'P99å»¶è¿Ÿ':<12}")
    print(f"{'-'*80}")
    
    for tier in SERVICE_TIERS:
        stats_key = f"{concurrency}_{tier}"
        stats = tier_stats[stats_key]
        pricing = "0.5x (50%â†“)" if tier == "flex" else "1.0x (åŸºå‡†)" if tier == "default" else "1.75x (75%â†‘)"
        
        if stats["latencies_ms"]:
            success_rate = stats['successful'] / stats['total_requests'] * 100
            avg_latency = mean(stats["latencies_ms"])
            latencies_sorted = sorted(stats["latencies_ms"])
            p95_idx = int(len(latencies_sorted) * 0.95)
            p99_idx = int(len(latencies_sorted) * 0.99)
            p95_latency = latencies_sorted[p95_idx]
            p99_latency = latencies_sorted[p99_idx]
            
            print(f"{tier:<12} {pricing:<15} {success_rate:<10.1f}% {avg_latency:<12.0f} {p95_latency:<12} {p99_latency:<12}")

# è¯¦ç»†ç»Ÿè®¡
print(f"\n{'='*80}")
print(f"è¯¦ç»†æ€§èƒ½æŒ‡æ ‡")
print(f"{'='*80}\n")

for concurrency in CONCURRENCY_LEVELS:
    print(f"\n{'='*80}")
    print(f"å¹¶å‘çº§åˆ«: {concurrency}")
    print(f"{'='*80}\n")
    
    for tier in SERVICE_TIERS:
        stats_key = f"{concurrency}_{tier}"
        stats = tier_stats[stats_key]
        pricing = "0.5x (50% æŠ˜æ‰£)" if tier == "flex" else "1.0x (åŸºå‡†)" if tier == "default" else "1.75x (75% æº¢ä»·)"
        
        print(f"## {tier.upper()} Tier - {pricing}")
        print(f"{'â”€'*80}")
        
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"  æˆåŠŸ: {stats['successful']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        print(f"  æˆåŠŸç‡: {stats['successful']/stats['total_requests']*100:.1f}%")
        print(f"  æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
        
        if stats["latencies_ms"]:
            latencies_sorted = sorted(stats["latencies_ms"])
            p95_idx = int(len(latencies_sorted) * 0.95)
            p99_idx = int(len(latencies_sorted) * 0.99)
            
            print(f"\nâš¡ Bedrock å»¶è¿Ÿç»Ÿè®¡ (æœåŠ¡ç«¯):")
            print(f"  å¹³å‡å»¶è¿Ÿ: {mean(stats['latencies_ms']):.0f}ms")
            print(f"  ä¸­ä½æ•°: {median(stats['latencies_ms']):.0f}ms")
            print(f"  P95: {latencies_sorted[p95_idx]}ms")
            print(f"  P99: {latencies_sorted[p99_idx]}ms")
            print(f"  æœ€å¿«: {min(stats['latencies_ms'])}ms")
            print(f"  æœ€æ…¢: {max(stats['latencies_ms'])}ms")
        
        if stats["input_tokens"]:
            print(f"\nğŸ”¤ Token ç»Ÿè®¡:")
            print(f"  è¾“å…¥ Token - å¹³å‡: {mean(stats['input_tokens']):.0f}")
            print(f"  è¾“å…¥ Token - æ€»è®¡: {sum(stats['input_tokens'])}")
            print(f"  è¾“å‡º Token - å¹³å‡: {mean(stats['output_tokens']):.0f}")
            print(f"  è¾“å‡º Token - æ€»è®¡: {sum(stats['output_tokens'])}")
            print(f"  æ€» Token: {sum(stats['input_tokens']) + sum(stats['output_tokens'])}")
        
        if stats["errors"]:
            print(f"\nâŒ é”™è¯¯ç»Ÿè®¡:")
            for error_msg, count in stats["errors"].items():
                print(f"  [{count}æ¬¡] {error_msg}")
        
        print()

# æ€§èƒ½å¯¹æ¯”åˆ†æ
print(f"{'='*80}")
print(f"æ€§èƒ½å¯¹æ¯”åˆ†æ")
print(f"{'='*80}\n")

for concurrency in CONCURRENCY_LEVELS:
    print(f"\n## å¹¶å‘çº§åˆ«: {concurrency}")
    print(f"{'â”€'*80}")
    
    stats_keys = [f"{concurrency}_{tier}" for tier in SERVICE_TIERS]
    if all(tier_stats[key]["latencies_ms"] for key in stats_keys):
        default_avg = mean(tier_stats[f"{concurrency}_default"]["latencies_ms"])
        flex_avg = mean(tier_stats[f"{concurrency}_flex"]["latencies_ms"])
        priority_avg = mean(tier_stats[f"{concurrency}_priority"]["latencies_ms"])
        
        print(f"âš¡ Bedrock å»¶è¿Ÿå¯¹æ¯” (ç›¸å¯¹äº Default):")
        print(f"  Flex:     {flex_avg:.0f}ms ({(flex_avg/default_avg-1)*100:+.1f}%)")
        print(f"  Default:  {default_avg:.0f}ms (åŸºå‡†)")
        print(f"  Priority: {priority_avg:.0f}ms ({(priority_avg/default_avg-1)*100:+.1f}%)")
        
        print(f"\nğŸ’° æ€§ä»·æ¯”åˆ†æ (é€Ÿåº¦/ä»·æ ¼):")
        # å»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥ç”¨ 1/latency
        flex_value = (1000/flex_avg) / 0.5
        default_value = (1000/default_avg) / 1.0
        priority_value = (1000/priority_avg) / 1.75
        
        print(f"  Flex:     {flex_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        print(f"  Default:  {default_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        print(f"  Priority: {priority_value:.2f} (æ€§ä»·æ¯”æŒ‡æ•°)")
        
        best_value = max(flex_value, default_value, priority_value)
        if best_value == flex_value:
            print(f"\nğŸ† æœ€ä½³æ€§ä»·æ¯”: Flex Tier")
        elif best_value == default_value:
            print(f"\nğŸ† æœ€ä½³æ€§ä»·æ¯”: Default Tier")
        else:
            print(f"\nğŸ† æœ€ä½³æ€§ä»·æ¯”: Priority Tier")
        
        print(f"\nğŸ“ˆ å¹¶å‘æ€§èƒ½è§‚å¯Ÿ:")
        print(f"  åœ¨ {concurrency} å¹¶å‘ä¸‹ï¼ŒPriority Tier çš„ä¼˜åŠ¿:")
        if priority_avg < default_avg:
            improvement = (1 - priority_avg/default_avg) * 100
            print(f"  - æ¯” Default å¿« {improvement:.1f}%")
            print(f"  - åœ¨é«˜è´Ÿè½½åœºæ™¯ä¸‹ï¼ŒPriority å¯ä»¥æä¾›æ›´ç¨³å®šçš„ä½å»¶è¿Ÿ")
        else:
            print(f"  - åœ¨å½“å‰è´Ÿè½½ä¸‹ï¼ŒPriority ä¼˜åŠ¿ä¸æ˜æ˜¾")

print(f"\n{'='*80}")
