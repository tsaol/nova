#!/usr/bin/env python3
"""
96å°æ—¶æŒç»­å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆå¢å¼ºç‰ˆï¼‰
- æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- è‡ªåŠ¨æ¢å¤æœºåˆ¶
- å¥åº·æ£€æŸ¥
- é”™è¯¯é‡è¯•
"""

import boto3
import json
import time
import csv
import signal
import sys
import threading
import argparse
import base64
from datetime import datetime, timedelta
from pathlib import Path
import pickle

# ====== é»˜è®¤é…ç½® ======
DEFAULT_REGION = "us-west-2"
DEFAULT_MODEL = "us.amazon.nova-2-lite-v1:0"
CONCURRENCY_LEVELS = [1, 5, 10]
HOURS_PER_LEVEL = 32
REQUEST_INTERVAL_SECONDS = 60
SERVICE_TIERS = ["flex", "default", "priority"]

# å›¾ç‰‡é…ç½®
TEST_IMAGE_PATH = Path(__file__).parent / "test_image.png"

# ==================

# å…¨å±€å˜é‡ï¼ˆåœ¨mainä¸­åˆå§‹åŒ–ï¼‰
AWS_REGION = None
MODEL_ID = None
DATA_DIR = None
CSV_FILE = None
STATE_FILE = None
client = None
running = True
TEST_IMAGE_BASE64 = None  # å›¾ç‰‡çš„base64ç¼–ç 

class TestState:
    """æµ‹è¯•çŠ¶æ€ç®¡ç†"""
    def __init__(self):
        self.current_concurrency_index = 0
        self.level_start_time = None
        self.batch_count = 0
        self.total_start_time = datetime.now()

    def save(self):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        with open(STATE_FILE, 'wb') as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls):
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'rb') as f:
                    return pickle.load(f)
            except:
                pass
        return cls()

def signal_handler(sig, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·"""
    global running
    print("\n\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜çŠ¶æ€...")
    running = False

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

def test_single_request_with_retry(tier, test_id, max_retries=3):
    """å¸¦é‡è¯•çš„å•æ¬¡è¯·æ±‚ï¼ˆä½¿ç”¨å›¾ç‰‡è¾“å…¥ï¼‰"""
    for attempt in range(max_retries):
        try:
            request_body = {
                "schemaVersion": "messages-v1",
                "messages": [{
                    "role": "user",
                    "content": [
                        {
                            "image": {
                                "format": "png",
                                "source": {
                                    "bytes": TEST_IMAGE_BASE64
                                }
                            }
                        },
                        {
                            "text": f"What do you see in this image? Test ID: {test_id}"
                        }
                    ]
                }],
                "inferenceConfig": {
                    "maxTokens": 100,
                    "temperature": 0.7
                }
            }

            invoke_params = {
                "modelId": MODEL_ID,
                "body": json.dumps(request_body),
                "contentType": "application/json",
                "accept": "application/json"
            }

            if tier != "default":
                invoke_params["serviceTier"] = tier

            start_time = time.time()
            response = client.invoke_model(**invoke_params)
            latency = int((time.time() - start_time) * 1000)

            model_response = json.loads(response["body"].read())
            usage = model_response.get("usage", {})
            http_headers = response.get("ResponseMetadata", {}).get("HTTPHeaders", {})
            server_latency = int(http_headers.get("x-amzn-bedrock-invocation-latency", 0))

            return {
                "success": True,
                "client_latency": latency,
                "server_latency": server_latency,
                "input_tokens": usage.get("inputTokens", 0),
                "output_tokens": usage.get("outputTokens", 0),
                "attempts": attempt + 1
            }

        except Exception as e:
            error_msg = str(e)

            # é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
            if "ThrottlingException" in error_msg or "429" in error_msg:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5, 10, 15 ç§’
                    print(f"    âš ï¸  é™æµï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                    continue

            # å…¶ä»–é”™è¯¯
            if attempt < max_retries - 1:
                time.sleep(2)
                continue

            return {
                "success": False,
                "error": error_msg,
                "attempts": max_retries
            }

    return {"success": False, "error": "Max retries exceeded"}

def test_concurrent_batch(tier, concurrency, batch_id):
    """æµ‹è¯•ä¸€æ‰¹å¹¶å‘è¯·æ±‚"""
    results = []
    lock = threading.Lock()

    def worker(worker_id):
        result = test_single_request_with_retry(tier, f"{tier}_{concurrency}_{batch_id}_{worker_id}")
        with lock:
            results.append(result)

    threads = []
    batch_start_time = time.time()

    for i in range(concurrency):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    batch_time = time.time() - batch_start_time

    # ç»Ÿè®¡
    successful = [r for r in results if r.get('success')]
    failed = len(results) - len(successful)

    if successful:
        avg_server = sum(r['server_latency'] for r in successful) / len(successful)
        avg_client = sum(r['client_latency'] for r in successful) / len(successful)
        avg_input = sum(r['input_tokens'] for r in successful) / len(successful)
        avg_output = sum(r['output_tokens'] for r in successful) / len(successful)
    else:
        avg_server = avg_client = avg_input = avg_output = 0

    return {
        "successful": len(successful),
        "failed": failed,
        "avg_server_latency": avg_server,
        "avg_client_latency": avg_client,
        "avg_input_tokens": avg_input,
        "avg_output_tokens": avg_output,
        "batch_time": batch_time
    }

def save_to_csv(data):
    """ä¿å­˜æ•°æ®"""
    file_exists = CSV_FILE.exists()

    with open(CSV_FILE, 'a', newline='') as f:
        fieldnames = ['timestamp', 'concurrency', 'tier', 'successful', 'failed',
                      'avg_server_latency', 'avg_client_latency',
                      'avg_input_tokens', 'avg_output_tokens', 'batch_time']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        writer.writerow(data)

def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        import shutil
        stat = shutil.disk_usage("/")
        free_gb = stat.free / (1024**3)

        if free_gb < 1:
            print(f"âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb:.2f}GB")
            return False

        # æ£€æŸ¥å†…å­˜
        with open('/proc/meminfo') as f:
            meminfo = f.read()
            for line in meminfo.split('\n'):
                if 'MemAvailable' in line:
                    mem_available_kb = int(line.split()[1])
                    mem_available_gb = mem_available_kb / (1024**2)
                    if mem_available_gb < 0.5:
                        print(f"âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³: {mem_available_gb:.2f}GB")
                        return False

        return True
    except:
        return True  # æ£€æŸ¥å¤±è´¥ä¸å½±å“æµ‹è¯•

def main():
    """ä¸»å‡½æ•°"""
    global AWS_REGION, MODEL_ID, DATA_DIR, CSV_FILE, STATE_FILE, client, TEST_IMAGE_BASE64

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='96å°æ—¶æŒç»­å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆå›¾ç‰‡è¾“å…¥ï¼‰')
    parser.add_argument('--region', default=DEFAULT_REGION, help=f'AWSåŒºåŸŸ (é»˜è®¤: {DEFAULT_REGION})')
    parser.add_argument('--model', default=DEFAULT_MODEL, help=f'æ¨¡å‹ID (é»˜è®¤: {DEFAULT_MODEL})')
    args = parser.parse_args()

    # è¯»å–å¹¶ç¼–ç æµ‹è¯•å›¾ç‰‡
    if not TEST_IMAGE_PATH.exists():
        print(f"âŒ é”™è¯¯: æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨: {TEST_IMAGE_PATH}")
        sys.exit(1)

    with open(TEST_IMAGE_PATH, 'rb') as f:
        image_bytes = f.read()
        TEST_IMAGE_BASE64 = base64.b64encode(image_bytes).decode('utf-8')

    print(f"âœ… å·²åŠ è½½æµ‹è¯•å›¾ç‰‡: {TEST_IMAGE_PATH} ({len(image_bytes)} bytes)")

    # åˆå§‹åŒ–å…¨å±€å˜é‡
    AWS_REGION = args.region
    MODEL_ID = args.model
    DATA_DIR = Path(f"./concurrent_96h_data_{AWS_REGION.replace('-', '_')}")
    DATA_DIR.mkdir(exist_ok=True)

    # CSVæ–‡ä»¶ååŒ…å«åŒºåŸŸä¿¡æ¯ï¼ˆæ ‡æ³¨ä¸ºimageæµ‹è¯•ï¼‰
    CSV_FILE = DATA_DIR / f"concurrent_96h_image_{AWS_REGION.replace('-', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    STATE_FILE = DATA_DIR / "test_state.pkl"

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = boto3.client("bedrock-runtime", region_name=AWS_REGION)

    print(f"\n{'='*80}")
    print(f"96å°æ—¶æŒç»­å¹¶å‘æ€§èƒ½æµ‹è¯•ï¼ˆå¢å¼ºç‰ˆ - æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰")
    print(f"{'='*80}")

    # å°è¯•æ¢å¤çŠ¶æ€
    state = TestState.load()

    if state.level_start_time:
        print(f"âœ… æ£€æµ‹åˆ°ä¹‹å‰çš„æµ‹è¯•çŠ¶æ€ï¼Œä»ä¸­æ–­å¤„ç»§ç»­...")
        print(f"   ä¸Šæ¬¡å¹¶å‘çº§åˆ«: {CONCURRENCY_LEVELS[state.current_concurrency_index]}")
        print(f"   ä¸Šæ¬¡æ‰¹æ¬¡: {state.batch_count}")
    else:
        print(f"ğŸ†• å¼€å§‹æ–°çš„æµ‹è¯•")
        state.total_start_time = datetime.now()

    print(f"å¼€å§‹æ—¶é—´: {state.total_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æµ‹è¯•åŒºåŸŸ: {AWS_REGION}")
    print(f"æµ‹è¯•æ¨¡å‹: {MODEL_ID}")
    print(f"å¹¶å‘çº§åˆ«: {CONCURRENCY_LEVELS}")
    print(f"æ•°æ®ä¿å­˜: {CSV_FILE}")
    print(f"çŠ¶æ€æ–‡ä»¶: {STATE_FILE}")
    print(f"{'='*80}\n")
    print("âœ¨ æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼šæµ‹è¯•ä¸­æ–­åå¯è‡ªåŠ¨æ¢å¤")
    print("âš¡ è‡ªåŠ¨é‡è¯•ï¼šé‡åˆ°é™æµè‡ªåŠ¨ç­‰å¾…é‡è¯•")
    print("ğŸ” å¥åº·æ£€æŸ¥ï¼šç›‘æ§ç£ç›˜å’Œå†…å­˜")
    print("\næŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢æµ‹è¯•\n")

    # ä»ä¿å­˜çš„ç´¢å¼•å¼€å§‹
    for conc_idx in range(state.current_concurrency_index, len(CONCURRENCY_LEVELS)):
        if not running:
            break

        concurrency = CONCURRENCY_LEVELS[conc_idx]
        state.current_concurrency_index = conc_idx

        # è®¾ç½®æˆ–æ¢å¤çº§åˆ«å¼€å§‹æ—¶é—´
        if state.level_start_time is None:
            state.level_start_time = datetime.now()
            state.batch_count = 0

        level_end_time = state.level_start_time + timedelta(hours=HOURS_PER_LEVEL)

        print(f"\n{'#'*80}")
        print(f"# å¹¶å‘çº§åˆ«: {concurrency}")
        print(f"# å¼€å§‹æ—¶é—´: {state.level_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"# é¢„è®¡ç»“æŸ: {level_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'#'*80}\n")

        while running and datetime.now() < level_end_time:
            state.batch_count += 1
            current_time = datetime.now()
            elapsed_hours = (current_time - state.level_start_time).total_seconds() / 3600
            progress = elapsed_hours / HOURS_PER_LEVEL * 100

            # å¥åº·æ£€æŸ¥ï¼ˆæ¯10æ‰¹æ¬¡ï¼‰
            if state.batch_count % 10 == 0:
                if not health_check():
                    print("âš ï¸  å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œæš‚åœ10ç§’...")
                    time.sleep(10)

            print(f"[{current_time.strftime('%H:%M:%S')}] å¹¶å‘ {concurrency} | "
                  f"æ‰¹æ¬¡ #{state.batch_count} | "
                  f"è¿›åº¦: {elapsed_hours:.1f}h/{HOURS_PER_LEVEL}h ({progress:.1f}%)")

            # æµ‹è¯•ä¸‰ä¸ª Tier
            for tier in SERVICE_TIERS:
                result = test_concurrent_batch(tier, concurrency, state.batch_count)

                save_to_csv({
                    'timestamp': current_time.isoformat(),
                    'concurrency': concurrency,
                    'tier': tier,
                    'successful': result['successful'],
                    'failed': result['failed'],
                    'avg_server_latency': result['avg_server_latency'],
                    'avg_client_latency': result['avg_client_latency'],
                    'avg_input_tokens': result['avg_input_tokens'],
                    'avg_output_tokens': result['avg_output_tokens'],
                    'batch_time': result['batch_time']
                })

                status = "âœ“" if result['failed'] == 0 else f"âš ï¸ {result['failed']}å¤±è´¥"
                print(f"  {tier:8} {status} {result['avg_server_latency']:4.0f}ms "
                      f"è€—æ—¶: {result['batch_time']:.1f}s")

            # ä¿å­˜çŠ¶æ€
            state.save()

            # ç­‰å¾…
            sleep_time = REQUEST_INTERVAL_SECONDS - (datetime.now() - current_time).total_seconds()
            if sleep_time > 0:
                time.sleep(sleep_time)

        # å®Œæˆå½“å‰çº§åˆ«ï¼Œé‡ç½®çŠ¶æ€
        print(f"\nâœ… å¹¶å‘çº§åˆ« {concurrency} å®Œæˆ\n")
        state.level_start_time = None
        state.batch_count = 0
        state.save()

    # æ¸…ç†çŠ¶æ€æ–‡ä»¶
    if STATE_FILE.exists():
        STATE_FILE.unlink()

    total_time = datetime.now() - state.total_start_time
    print(f"\n{'='*80}")
    print(f"âœ… æµ‹è¯•å®Œæˆ")
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time.total_seconds()/3600:.1f} å°æ—¶")
    print(f"æ•°æ®æ–‡ä»¶: {CSV_FILE}")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­ï¼ŒçŠ¶æ€å·²ä¿å­˜")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ çŠ¶æ€å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­æµ‹è¯•")
